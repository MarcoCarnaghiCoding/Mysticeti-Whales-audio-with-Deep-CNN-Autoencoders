{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODERS\n",
    "\n",
    "The objective of these scripts is to develop different alternatives for Autoencoder trained with spectrograms. More specifically, the spectrograms will come from audio signals that will be recordings of different scenarios related to the sea, such as: animals, ships, sea noises, etc. Each spectrogram is a 2-dimensional matrix, which can be thought of as a black and white image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AutoEncoder v1\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "#print(tf.__version__)==> VERSION DE TENSORFLOW  1.10\n",
    "import matplotlib\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten, Dense, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Activation, Lambda, LSTM\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import mean_squared_error # MeanSquaredError\n",
    "from tensorflow.keras import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle   #Lo tuve que agregar nuevamente porque me decÃ­a que no estaba definido cuando guarda los datos\n",
    "import skimage   #la version que se instla en conda es la 0.14.0 ==> hay que ver nombre de la funcion para metricas\n",
    "from skimage import measure \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in files with extension npy (numpy). The file returns a 3-dimensional tensor (FxTxN) , where N is the number of files, F represents the rows and T the columns. \n",
    "\n",
    "There are two more files: frequency and time, which are used to visualize the data.\n",
    "\n",
    "The data values are expressed in dB since the logarithmic scale allows a better visualization.\n",
    "\n",
    "The data was previously normalized in both frequency and time. \n",
    "\n",
    "The re-sample frequency is 44100 Hz and the time of each spectrogram is 1 sec. \n",
    "\n",
    "The spectrograms are obtained from audio normalized between -1 and 1. \n",
    "\n",
    "The number of samples of the FFT=256 The returned values are between -150dB and 50dB.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING OF THE DATA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pxx=np.load('.../PECESlog.npy')            #windows \n",
    "#Pxx=np.load('.../ballena_jorobada.npy')    #windows \n",
    "#Pxx=np.load('.../blue_whale.npy')          #windows\n",
    "Pxx=np.load('.../Bowhead_whale.npy')        #windows\n",
    "#Pxx=np.load('.../MYSTICETlog.npy')         #windows \n",
    "#Pxx=np.load('.../MYSTICETlog.npy')         #\"linux\"\n",
    "print(np.shape(Pxx)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length of Data\n",
    "\n",
    "The dimentions of the tensors are forced to pair values, in order to make them more appropiated for the 2D convolutions performed during the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_col=len(Pxx[1,:,1])\n",
    "len_row=len(Pxx[:,1,1])\n",
    "\n",
    "if (len_col % 2) !=0 :\n",
    "   Pxx = Pxx[:,0:-1, :]\n",
    "if (len_row % 2) !=0 :\n",
    "   Pxx = Pxx[0:-1,:, :] \n",
    "\n",
    "print (np.shape(Pxx)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization between 0 and 1\n",
    "\n",
    "This normalization is needed as a consequence of the function definition in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalizationSpectograms :\n",
    "    def __init__ (self, input_data):\n",
    "        self.np_data = np.array(input_data)\n",
    "        self.min = np.min(self.np_data)\n",
    "        self.max = np.max(self.np_data)\n",
    "    \n",
    "    def spectograms_to_image (self,input_data):\n",
    "        self.norm_array = ( self.np_data - self.min)/(self.max-self.min)\n",
    "        return self.norm_array\n",
    "\n",
    "P_normalizer = MinMaxNormalizationSpectograms(Pxx)\n",
    "p_train_norm = P_normalizer.spectograms_to_image(Pxx)\n",
    "\n",
    "#input_shape=np.shape(x_train_norm)\n",
    "print(np.shape(p_train_norm ))\n",
    "#print(input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "Since our dataset has no temporal order, it is convenient to perform the splitting randomly to ensure maximum variability of the data in our train/test sets. This can be done with a specific library for this purpose: scikit-learn: train_test_split()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=len((p_train_norm[:,1,1])) #Rows\n",
    "b=len((p_train_norm[1,:,1])) #Cols\n",
    "c=len((p_train_norm[1,1,:])) #Files \n",
    "\n",
    "Pxx=np.reshape(p_train_norm,(c,a,b, 1))\n",
    "print(np.shape(Pxx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(Pxx, test_size = 0.10)\n",
    "print(np.shape(x_test))\n",
    "print(np.shape(x_train))\n",
    "\n",
    "\n",
    "input_shape=np.shape(x_train)\n",
    "print(input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder:\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape,   # Dimension of the input data. In this case, 2D spectrograms.\n",
    "                 conv_filters,  # Number of convolution filters for each layer.\n",
    "                 conv_kernels,  # Dimensions of the convolution filters. By itself,\n",
    "                                # these filters are intended to be 2D because the input \n",
    "                                # is 2D and will be defined as such. \n",
    "                                # In this case, the dimensions of the rows=columns.\n",
    "                 conv_strides,  # The shift in the convolution of the filter with the input.\n",
    "                 latent_space_dim #Dimension of the latent space used as bottleneck between encoder and decoder.\n",
    "                 ): \n",
    "   \n",
    "        self.input_shape        = input_shape # Each individual entry will have the dimension [196,128,1].  \n",
    "        self.conv_filters       = conv_filters # [2, 4, 8] \n",
    "        self.conv_kernels       = conv_kernels # [3, 5, 3] ==> (3x3x1,5x5x1,3x3x1)\n",
    "        self.conv_strides       = conv_strides # [1, 2, 2]\n",
    "        self.latent_space_dim   = latent_space_dim #\n",
    "        self.Encoder            = None\n",
    "        self.Decoder            = None\n",
    "        self.Model              = None\n",
    "        self._model_input       = None\n",
    "        self._num_conv_layers   = len(conv_filters) # Conv layers in the model\n",
    "        self._shape_before_bottleneck = None\n",
    "           \n",
    "        self._build() \n",
    "\n",
    "  \n",
    "    def _build(self):\n",
    "        self._build_encoder() \n",
    "        self._build_decoder()  \n",
    "        self._build_autoencoder() \n",
    "\n",
    "  #----------------------------------------------------------------------------------------\n",
    "  #                   ENCODER\n",
    "  #---------------------------------------------------------------------------------------\n",
    "  \n",
    "    def _build_encoder(self):\n",
    "        encoder_input = self._add_encoder_input() #Add input layer\n",
    "        conv_layers = self._add_conv_layers(encoder_input) #Add all conv layers\n",
    "        bottleneck = self._add_bottleneck(conv_layers) #Agrega el cuello de botella\n",
    "        self._model_input = encoder_input \n",
    "        self.Encoder = Model(encoder_input,bottleneck, name='Encoder_v1') \n",
    "        \n",
    "    def _add_encoder_input(self):\n",
    "        return Input(\n",
    "            shape = self.input_shape,\n",
    "            name = \"Encoder_Input_layer\"\n",
    "            )\n",
    "\n",
    "    def _add_conv_layers(self, temp_model):\n",
    "        #Creation of all convolutional layers in the model.#\n",
    "        aux = temp_model\n",
    "        #Add layer to layer to the model\n",
    "        for layer in range(self._num_conv_layers):\n",
    "            aux = self._add_conv_layer(layer,aux)\n",
    "        return aux\n",
    "\n",
    "    def _add_conv_layer(self, layer, temp_model):\n",
    "        #Add convolutional layers to the model (temp_model). Each convolutional layer consists of:\n",
    "        #- Conv2D: convolution with 2D filters.\n",
    "        #- ReLU : activation function applied to the convolution result.\n",
    "        #- BatchNromalzation : This operation is added as a Subsampling or Pooling layer (parameter reduction layer). This operation accelerates the training process.\n",
    "            # Each layer of a neural network has inputs with a corresponding distribution, which is affected during the training process by \n",
    "            # Randomness in parameter initialization and randomness in the input data. \n",
    "            # The effect of these sources of randomness on the distribution of the inputs produces changes in the means and variances of the inputs to the inner layers during training. \n",
    "            # to the inner layers during training. \n",
    "            # During the training stage of the networks, as the parameters of the previous layers change, the distribution of inputs to the layer changes,\n",
    "            # so that the current layer needs to constantly readjust to the new distributions. This problem increases in deep networks, \n",
    "            # since small changes in shallower hidden layers will be amplified as they propagate within the network. \n",
    "            # Therefore, the batch normalization method is proposed to reduce these unwanted changes to speed up training and to produce more reliable models. \n",
    "            # produce more reliable models.\n",
    "            # Batch normalization seems to have a regularization effect, so that the network improves its generalization properties.\n",
    "            # It has also been observed that, with batch standardization, the network becomes more robust to different initialization schemes and learning rates.\n",
    "        #- The padding is adjusted to obtain at the output of each convolution, the appropriate matrices.     \n",
    "        conv_layer = Conv2D(\n",
    "            filters=self.conv_filters[layer],\n",
    "            kernel_size=self.conv_kernels[layer],\n",
    "            strides=self.conv_strides[layer],\n",
    "            padding=\"same\", \n",
    "            name=\"encoder_conv_layer_\"+str(layer),\n",
    "            trainable=False\n",
    "            )\n",
    "        temp_model = conv_layer(temp_model)\n",
    "        temp_model = ReLU(name=\"encoder_ReLU_layer_\"+str(layer))(temp_model)\n",
    "        temp_model = BatchNormalization(name=\"encoder_BN_layer_\"+str(layer))(temp_model)\n",
    "        return temp_model\n",
    "\n",
    "    def _add_bottleneck(self, temp_model):\n",
    "        #- A dense (fully connected) layer is added to the model to obtain the latent space.\n",
    "        #- A Flatten layer must be added to resha.pe the output tensor of the convolutional layers to a vector suitable for a Dense layer.\n",
    "        #- Information about the shape of the model is stored, before storing the latent space, for the duplication process while the decoder is being built. \n",
    "        #- The model is stored for replication in the decoder\n",
    "        self._shape_before_bottleneck = K.int_shape(temp_model)[1:]\n",
    "        temp_model = Flatten()(temp_model) \n",
    "        temp_model = Dense(self.latent_space_dim, name=\"Decoder_output\")(temp_model)\n",
    "        return temp_model\n",
    "\n",
    "  #------------------------------------------------------------------------------------------------------------------------------\n",
    "  #                   DECODER\n",
    "  #------------------------------------------------------------------------------------------------------------------------------\n",
    "    def _build_decoder(self):\n",
    "        decoder_input = self._add_decoder_input()\n",
    "        dense_layer = self._add_dense_layer(decoder_input)\n",
    "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
    "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
    "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
    "        self.Decoder = Model(decoder_input, decoder_output, name=\"Decoder_v1\")\n",
    "\n",
    "    def _add_decoder_input(self):\n",
    "        return Input(\n",
    "            shape = (self.latent_space_dim,),  \n",
    "            name = \"Decoder_input_layer\"\n",
    "        )\n",
    "\n",
    "    def _add_dense_layer(self, temp_model):\n",
    "        dense_layer = Dense(\n",
    "            np.prod(self._shape_before_bottleneck),\n",
    "            name = \"Decoder_dense_layer\"\n",
    "            )(temp_model)\n",
    "        return dense_layer\n",
    "\n",
    "    def _add_reshape_layer(self,temp_model):\n",
    "        return Reshape(self._shape_before_bottleneck)(temp_model)\n",
    "\n",
    "    def _add_conv_transpose_layers(self, temp_model):\n",
    "        # Add all the transpose convolutional layers to the model\"\"\"\n",
    "        for layer in reversed(range( 1, self._num_conv_layers)):\n",
    "              temp_model = self._add_conv_transpose_layer(layer,temp_model)\n",
    "        return temp_model\n",
    "\n",
    "    def _add_conv_transpose_layer(self,layer,temp_model):\n",
    "        #\"Add a single transpose conv layer to the model \"\"\"\n",
    "        layer_num = self._num_conv_layers - layer\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters = self.conv_filters[layer],\n",
    "            kernel_size=self.conv_kernels[layer],\n",
    "            strides=self.conv_strides[layer],\n",
    "            padding=\"same\",\n",
    "            name=\"decoder_conv_transpose_layer_\"+str(layer), \n",
    "            trainable=False\n",
    "            )\n",
    "        temp_model = conv_transpose_layer(temp_model)\n",
    "        temp_model = ReLU(name =\"decoder_ReLU_transpose_layer_\"+str(layer))(temp_model)\n",
    "        temp_model = BatchNormalization(name = \"decoder_BN_transpose_layer_\"+str(layer))(temp_model)\n",
    "        return temp_model\n",
    "\n",
    "    def _add_decoder_output(self, temp_model):\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters=1,\n",
    "            kernel_size=self.conv_kernels[0],\n",
    "            strides=self.conv_strides[0],\n",
    "            padding=\"same\",\n",
    "            name=\"decoder_conv_transpose_layer_\"+str(self._num_conv_layers)\n",
    "            )\n",
    "        temp_model = conv_transpose_layer(temp_model)\n",
    "        output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(temp_model)\n",
    "        return output_layer\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   AUTOENCODER\n",
    "  #----------------------------------------------\n",
    "    def _build_autoencoder(self):\n",
    "        model_input = self._model_input\n",
    "        model_output = self.Decoder(self.Encoder(model_input))\n",
    "        self.Model = Model(model_input, model_output, name=\"AutoEncoder_CNN\")\n",
    "\n",
    "  #----------------------------------------------------------------------------------------------------------------------\n",
    "  #            COMPILATION AND TRAINING\n",
    "  #----------------------------------------------------------------------------------------------------------------------\n",
    "    # When compiling, we must specify some additional properties necessary for training the network. \n",
    "    # Training a network means finding the best set of weights for the network to do what it should do. \n",
    "    # We must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network. \n",
    "    # and if we wanted we could add any optional metrics we would like to collect and report during training.\n",
    "    # For training we use the mean square error as a loss function and a type of optimizer called Adam.\n",
    "    # Adamm is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.\n",
    "    \n",
    "        def compile(self, learning_rate=0.0001):\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        mse_loss = mean_squared_error\n",
    "        self.Model.compile(optimizer=optimizer, loss=mse_loss)\n",
    "    \n",
    "    #We have defined our model and compiled it ready for efficient calculation. Now it is time to run the model on some data. \n",
    "    #We can train or fit our model to the loaded data by calling the fit() function in the model.\n",
    "    # The training process will run for a fixed number of iterations called epochs or epochs. \n",
    "    # We can also set the number of instances that are evaluated before a weight update is performed on the network called batch_size. \n",
    "    # For this problem we will use a small number of epochs (150) and a relatively small batch_size (10). \n",
    "    # These can be chosen experimentally by trial and error.\n",
    "    # This function returns a History attribute. If, instead of using for validation a percentage of the data,\n",
    "    #I want to use data, then, I should change percentage_validation to data_validation=(x_test,x_test). That is to say that you should put the test data and the target.\n",
    "    #test data and the target.\n",
    "    def train(self, x_train, batch_size, num_epochs,porcentaje_validacion):\n",
    "        History = self.Model.fit(x_train,\n",
    "                                x_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=num_epochs,\n",
    "                                validation_split=porcentaje_validacion, #Indica el porcentaje de datos de train que se usarÃ¡n para validaciÃ³n\n",
    "                                shuffle=True)\n",
    "        return History\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   EVALUATION \n",
    "  #----------------------------------------------   \n",
    "    #This function returns the loss value and the metric values of the model in test mode.\n",
    "    def evaluation(self,x_test):  \n",
    "        evaluacion=self.Model.evaluate(\n",
    "               x=x_test,\n",
    "               y=x_test,\n",
    "               verbose=1\n",
    "               )\n",
    "        return evaluacion\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   STRUCTURE SUMMARY\n",
    "  #----------------------------------------------\n",
    "\n",
    "    def summary(self):\n",
    "        self.Encoder.summary()\n",
    "        self.Decoder.summary()\n",
    "        self.Model.summary()\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                  STORE THE MODEL\n",
    "  #----------------------------------------------\n",
    "    def save(self, save_folder=\".\"):\n",
    "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
    "        self._save_parameters(save_folder)\n",
    "        self._save_weights(save_folder)\n",
    "\n",
    "    def _create_folder_if_it_doesnt_exist(self, folder):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "    def _save_parameters(self, save_folder):\n",
    "            parameters = [\n",
    "              self.input_shape,\n",
    "              self.conv_filters,\n",
    "              self.conv_kernels,\n",
    "              self.conv_strides,\n",
    "              self.latent_space_dim\n",
    "              ]\n",
    "            save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                pickle.dump(parameters, f)\n",
    "\n",
    "    def _save_weights(self, save_folder):\n",
    "        save_path = os.path.join(save_folder, \"weights.h5py\")  \n",
    "        self.Model.save_weights(save_path)\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                  LOAD THE MODEL\n",
    "  #----------------------------------------------\n",
    "    def load_weights(self, weights_path):\n",
    "        self.Model.load_weights(weights_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, save_folder=\".\"):\n",
    "        parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
    "        with open(parameters_path, \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "        autoencoder = AutoEncoder(*parameters)\n",
    "        weights_path = os.path.join(save_folder, \"weights.h5py\") #En versiones posteriores, la extensiÃ³n podrÃ­a ser .h5\n",
    "        autoencoder.load_weights(weights_path)\n",
    "        return autoencoder\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   PREDICTION\n",
    "  #----------------------------------------------\n",
    "  #Reconstruction of the input images.\n",
    "  #This function returns a 2-dimensional tensor:\n",
    "  # - In dimension 1 it will reconstruct a spectrogram matrix, starting from a matrix (data) at the input.\n",
    "  # The shape of the input data will be a 4-dimensional tensor. The output will also be a 4-dimensional tensor.\n",
    "  # I.e., if input with n number of images (n,x,y,1), the output will be a tensor (n,x1,y1,1).\n",
    "  # In dimension 2, it returns the latent space tensor. In this type of convolutionals, the latent space is discrete, \n",
    "  # therefore it will return a vector for each image entered. \n",
    "\n",
    "  #It is used with some test data to corroborate the similarity, then some kind of error could be calculated.\n",
    "  #The latent space is found at position 1 of the tensor.\n",
    "    def reconstruct(self, images):\n",
    "        latent_representations = self.Encoder.predict(images)\n",
    "        reconstructed_images = self.Decoder.predict(latent_representations)\n",
    "        return reconstructed_images, latent_representations\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture has a coding section based on convolutional networks, in which a feature extraction section and a densely connected flatten layer are performed to obtain a representative matrix or generator code. From this generator code and replicating the coding process in reverse, an attempt is made to replicate the input spectrogram.\n",
    "\n",
    "The proposed architecture has 4 layers in the feature extraction stage. In all stages the activation function is RELU type and the padding, associated to the filtering stage, is automatically rearranged so that at the output of the filtering stage, the size of the rows and columns is an integer. A BachNormalization layer is added as a Pooling layer. A Flatten layer is added to obtain a representative data vector.\n",
    "\n",
    "* 1st layer: 32 Filters of 3x3x1. The Stride will be 1 and the padding will be adjusted.\n",
    "* 2nd layer: 64 Filters of 3x3x1. The Stride will be 2 and the padding is accommodated.\n",
    "* 3rd layer: 64 Filters of 3x3x1. The Stride will be 2 and the padding is accommodated.\n",
    "* 4th layer: 64 Filters of 3x3x1. The Stride will be 1 and the padding will be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_CNN = AutoEncoder(\n",
    "            input_shape=(input_shape[1],input_shape[2], 1), \n",
    "            conv_filters=(32, 64, 64, 64),  \n",
    "            conv_kernels=(3, 3, 3, 3),  \n",
    "            conv_strides=(1, 2, 2, 1), \n",
    "            latent_space_dim = 4, \n",
    "            )\n",
    "#autoencoder_CNN.Model.layers[1].trainable = False\n",
    "autoencoder_CNN.summary()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_path = ''\n",
    "autoencoder_CNN = AutoEncoder.load(autoencoder_path)  #WINDOWS\n",
    "#autoencoder2.summary()  #show model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 70\n",
    "EPOCHS = 100\n",
    "\n",
    "percent_validation=0.2 \n",
    "\n",
    "#COMPILATION AND TRAINING\n",
    "#At the compilation stage, additional properties required for training the network are specified. \n",
    "# The loss function to be used to evaluate a set of weights and the optimizer used to search through different weights for the network, \n",
    "# are already defined above and are: \n",
    " # - the mean square error as the loss function. \n",
    " # - the Adam optimizer. Adam is a replacement optimization algorithm for stochastic gradient descent to train deep learning models.\n",
    "# It only remains to specify the learning rate.\n",
    "autoencoder_CNN.compile(LEARNING_RATE)\n",
    "\n",
    "# TRAINING\n",
    " # Training a network means finding the best set of weights for the network to do what it should do. It is time to run the model on some data. \n",
    "    # The training process will run for a fixed number of iterations called epochs or epochs. \n",
    "    # The number of instances that are evaluated before a weight update is performed on the network called batch_size. \n",
    "    # These can be chosen experimentally by trial and error.\n",
    "    # the validation percentage, indicates what percentage of the train data is used to validate.\n",
    "#This function returns a History attribute. If, instead of using a percentage of the data for validation, I wanted to use data, then, instead of using a percentage of the data, I would use data,\n",
    "#I want to use data, then, I should change percentage_validation to data_validation=(x_test,x_test). That is to say that you should put the test data and the target.\n",
    "#test data and the target.\n",
    "History=autoencoder_CNN.train(x_train,BATCH_SIZE,EPOCHS,percent_validation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store Model\n",
    "#autoencoder_CNN.save(\".../modelo_CNN\")  #WINDOWS\n",
    "autoencoder_CNN.save(\".../CNN_transfer_bowhead100\")\n",
    "\n",
    "#Store Error vector\n",
    "error_train=History.history[\"loss\"]\n",
    "error_train_save        =   np.save('...vectores_error/train_CNN_bowhead_transfer100.npy', error_train)\n",
    "\n",
    "error_validation        =   History.history[\"val_loss\"]\n",
    "error_validation_save   =   np.save('.../vectores_error/val_CNN_bowhead_transfer100.npy',error_validation)\n",
    "\n",
    "#Load Train vectors\n",
    "error_train_save        =   np.save('.../vectores_error/train_CNN_franca_transfer100.npy', error_train)\n",
    "error_validation_save   =   np.save('.../vectores_error/val_CNN_franca_transfer100.npy',error_validation)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(\"DATOS DE ENTRENAMIENTO\")\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('Ã©pocas')\n",
    "plt.plot(History.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(History.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION OF THE FINAL IMAGE WITH TEST DATA\n",
    "In this part we will evaluate with some test data, the similarity between the entered images and the obtained ones. Two different metrics will be used for the evaluation:\n",
    "\n",
    "SSIM index (structural similarity index measure). SSIM is used to measure the similarity between two images.\n",
    "The SSIM index is a full reference metric; in other words, the measurement or prediction of image quality is based on an initial uncompressed or undistorted image as a reference. SSIM is a perception-based model that considers image degradation as a perceived change in structural information, while incorporating important perceptual phenomena, including the terms luminance masking and contrast masking. The difference with other techniques such as MSE or PSNR is that these approaches estimate absolute errors. Structural information is the idea that pixels have strong interdependencies, especially when they are spatially close. These dependencies carry important information about the structure of objects in the visual scene.\n",
    "\n",
    "Mean square error,\n",
    "Adequacy of the original TEST image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N               =   10\n",
    "test_image      =   x_test[N,:]                   \n",
    "original_image  =   np.float32(test_image)\n",
    "\n",
    "\n",
    "result          =   autoencoder_CNN.reconstruct(x_test)  \n",
    "aux             =   result[0]  #array of output images (n,128, 196, 1) for test data.\n",
    "output_image    =   aux[N,:]  #Reconstruction of the one image. Dimension is: (128, 196, 1)\n",
    "\n",
    "# # Mean squared error\n",
    "MSE=skimage.measure.compare_mse(original_image,output_image)\n",
    "print('MSE:',MSE)\n",
    "\n",
    "# # SSIM\n",
    "SSIM=skimage.measure.compare_ssim(original_image,output_image,multichannel=True)\n",
    "print('SSIM:',SSIM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT SPECTROGRAMS\n",
    "#Time axes are loaded. These data can be obtained from sampling frequency and time, but are obtained directly from memory. \n",
    "# They are generated when the spectrograms are made.\n",
    "time  =   np.load('.../frecuencia.npy')\n",
    "freq  =   np.load('.../tiempo.npy')\n",
    "\n",
    "#The size of these axes is adapted to the length of the images.\n",
    "time1     =   time[0:128]\n",
    "freq1     =   freq[0:196]\n",
    "\n",
    "#La matriz que se debe pasar para el espectograma debe ser de 2 dimensiones.\n",
    "original        = original_image[:,:,0]     \n",
    "reconstruct     = output_image[:,:,0]   \n",
    "\n",
    "#Create Figure\n",
    "plt.figure(figsize=(12,4),dpi=150)\n",
    "plt.subplot(1,2,1)\n",
    "plt.text(0.8,26000,\"BALLENA DE GREONLANDIA\", fontsize=12)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "L=len(x_train[:,1,1])\n",
    "plt.text(0.8,-4100,f'MSE={MSE:.5f} ; SSIM={SSIM:.5f}); N= {L}',fontsize=10)\n",
    "plt.title(\"Original Spectrogram TEST\")\n",
    "Pxx_original=plt.pcolormesh(freq1, time1, original, cmap='Spectral')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.title(\"Reconstructed Spectrogram TEST\")\n",
    "Pxx_reconstruc=plt.pcolormesh(freq1, time1, reconstruct, cmap='Spectral')\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Synthetic images\n",
    "\n",
    "At this point, the aim is to obtain a synthetic image from the code generated by the autoencoder. Actually, with this model a code is generated for each trained image. One should choose one at random and generate a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The generated code has as many subcodes as data were trained in the autoecoder and a similar image will be obtained.\n",
    "# The subcodes are discrete, so I must send you a particular code. \n",
    "#The choice of the entered code could be done randomly.\n",
    "import random\n",
    "\n",
    "code    =   result[1]\n",
    "print(np.shape(code))\n",
    "M       =   random.randint(0, len(code[:,0]))\n",
    "print(M)\n",
    "unique_code    =   code[M,:]\n",
    "print(unique_code)\n",
    "print(np.shape(unique_code),type(unique_code))\n",
    "\n",
    "unique_tensor       =   tf.expand_dims(unique_code, axis=0) #Esta funciÃ³n me devuelve un tensor, asique se debe pasar a array.\n",
    "unique_code_final   =   tf.compat.v1.Session().run(unique_tensor)\n",
    "\n",
    "Image_final = autoencoder_CNN.Decoder.predict(unique_code_final)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE AUTOENCODER\n",
    "\n",
    "The first improvement that can be made to the previous version of AutoEncoder is to use a variational AutoEncoder approach.\n",
    "\n",
    "The main change from the standard AutoEncoder is that VAE uses the TRIAN ensemble to develop a multidimensional distribution that is continuous in latent space. The mapping to single points in the latent space, which CNN does makes it difficult to generate new cohesive outputs when the latent space when the input is relatively far from the known points belonging to the TRAIN set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "  \"\"\"\n",
    "  The present class refers to a Deep CNN Variational AutoEncoder architecture\n",
    "  where the encoder and the decoder components are mirrored.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "                 input_shape,\n",
    "                 conv_filters,\n",
    "                 conv_kernels,\n",
    "                 conv_strides,\n",
    "                 latent_space_dim):\n",
    "    self.input_shape = input_shape # [28, 28, 1]\n",
    "    self.conv_filters = conv_filters # [2, 4, 8]\n",
    "    self.conv_kernels = conv_kernels # [3, 5, 3]\n",
    "    self.conv_strides = conv_strides # [1, 2, 2]\n",
    "    self.latent_space_dim = latent_space_dim # 2\n",
    "    self.reconstruction_loss_weight = 1000\n",
    "    self.Encoder = None\n",
    "    self.Decoder = None\n",
    "    self.Model = None\n",
    "    self._model_input = None\n",
    "    self._num_conv_layers = len(conv_filters) #Number of convolutional layers in the Model\n",
    "    self._shape_before_bottleneck = None\n",
    "\n",
    "    self._build()\n",
    "  \n",
    "  def _build(self):\n",
    "    self._build_encoder() #Method to create the Encoder model\n",
    "    self._build_decoder()\n",
    "    self._build_autoencoder() \n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   ENCODER\n",
    "  #----------------------------------------------\n",
    "  def _build_encoder(self):\n",
    "    encoder_input = self._add_encoder_input() #Add input layer\n",
    "    conv_layers = self._add_conv_layers(encoder_input) #Add all conv layers\n",
    "    bottleneck = self._add_bottleneck(conv_layers) \n",
    "    self._model_input = encoder_input \n",
    "    self.Encoder = Model(encoder_input, bottleneck, name='Encoder_VAE')\n",
    "\n",
    "  def _add_encoder_input(self):\n",
    "    return Input(\n",
    "        shape = self.input_shape,\n",
    "        name = \"Encoder_Input_layer\"\n",
    "    )\n",
    "\n",
    "  def _add_conv_layers(self, temp_model):\n",
    "    \"\"\"Creation of all convolutional layers in the model.\"\"\"\n",
    "    aux = temp_model\n",
    "\n",
    "    #Add layer to layer to the model\n",
    "    for layer in range(self._num_conv_layers):\n",
    "      aux = self._add_conv_layer(layer,aux)\n",
    "      \n",
    "    return aux\n",
    "\n",
    "  def _add_conv_layer(self, layer, temp_model):\n",
    "    \"\"\"\n",
    "    Add a single convolutional layer to the model (temp_model)\n",
    "    Each convolutional layer consist on:\n",
    "    Conv2D: convolution with filters\n",
    "    ReLU : activation function applied to the result of the convolution\n",
    "    BatchNromalzation : Operation included to speed up the training process\n",
    "\n",
    "    :param: layer: the actual layer index in the model building process\n",
    "    :param: temp_model: the in-developing model to which the conv layer will be added \n",
    "    \"\"\"\n",
    "    conv_layer = Conv2D(\n",
    "        filters=self.conv_filters[layer],\n",
    "        kernel_size=self.conv_kernels[layer],\n",
    "        strides=self.conv_strides[layer],\n",
    "        padding=\"same\",\n",
    "        name=f\"encoder_conv_layer_{layer}\"\n",
    "        )\n",
    "    temp_model = conv_layer(temp_model)\n",
    "    temp_model = ReLU(name=f\"encoder_ReLU_layer_{layer}\")(temp_model)\n",
    "    temp_model = BatchNormalization(name=f\"encoder_BN_layer_{layer}\")(temp_model)\n",
    "    return temp_model\n",
    "    \n",
    "  #ESPACIO LATENTE  \n",
    "    \"\"\"\n",
    "    Se agrega una capa densa (totalmente conectada) al modelo para obtener el espacio latente.\n",
    "    Esta capa (Flatten) remodela el tensor de salida de las capas convolucionales a un vector adecuado.\n",
    "    \"\"\"\n",
    "  def _add_bottleneck(self, temp_model):\n",
    "    self._shape_before_bottleneck = K.int_shape(temp_model)[1:]   #InformaciÃ³n sobre la forma del modelo antes de almacenar el espacio latente. Sirve para el proceso de duplicaciÃ³n al  construir el Decodificador.\n",
    "    temp_model = Flatten()(temp_model)\n",
    "\n",
    "    #El espacio latente estÃ¡ envuelto por una distribuciÃ³n normal. La distribuciÃ³n normal estÃ¡ completamente definida por su valor medio y su desviaciÃ³n\n",
    "    self.mu = Dense(self.latent_space_dim, name = \"mu_latent_space\")(temp_model)\n",
    "    self.log_variance = Dense(self.latent_space_dim, name = \"log_variance_latent_space\")(temp_model)\n",
    "    \n",
    "    #DefiniciÃ³n de la distribuciÃ³n normal. Esta sera la funcion de transformacion\n",
    "    def map_to_normal_distribution(args):  \n",
    "      mu, log_variance = args   #me devuelve la media y el logaritmo de la varianza\n",
    "      base_gaussian = K.random_normal(\n",
    "        shape = K.shape(self.mu),\n",
    "        mean = 0.,\n",
    "        stddev = 1.\n",
    "      )\n",
    "      point = mu + K.exp(log_variance/2) * base_gaussian # la desviacion standar es la raiz cuadrada de la varianza\n",
    "      return point\n",
    "\n",
    "    #Las capas Lambda se utilizan para implementar capas con una funciÃ³n artesanal.\n",
    "    # En este caso, la funciÃ³n seguirÃ¡ la distribuciÃ³n normal parametrizada\n",
    "    # cuyos parÃ¡metros deben ser entrenados\n",
    "\n",
    "    temp_model = Lambda(\n",
    "        map_to_normal_distribution,\n",
    "        name = \"Encoder_output\")([self.mu,self.log_variance])\n",
    "\n",
    "    return temp_model\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   DECODER\n",
    "  #----------------------------------------------\n",
    "  def _build_decoder(self):\n",
    "    decoder_input = self._add_decoder_input()\n",
    "    dense_layer = self._add_dense_layer(decoder_input)\n",
    "    reshape_layer = self._add_reshape_layer(dense_layer)\n",
    "    conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
    "    decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
    "    self.Decoder = Model(decoder_input, decoder_output, name=\"Decoder_VAE\")\n",
    "\n",
    "  def _add_decoder_input(self):\n",
    "    return Input(\n",
    "        shape = (self.latent_space_dim,),\n",
    "        name = \"Decoder_input_layer\"\n",
    "    )\n",
    "\n",
    "  def _add_dense_layer(self, temp_model):\n",
    "    dense_layer = Dense(\n",
    "        np.prod(self._shape_before_bottleneck),\n",
    "        name = \"Decoder_dense_layer\"\n",
    "    )(temp_model)\n",
    "    return dense_layer\n",
    "\n",
    "  def _add_reshape_layer(self,temp_model):\n",
    "    return Reshape(self._shape_before_bottleneck)(temp_model)\n",
    "\n",
    "  def _add_conv_transpose_layers(self, temp_model):\n",
    "    \"\"\" Add all the transpose convolutional layers to the model\"\"\"\n",
    "    for layer in reversed(range( 1, self._num_conv_layers)):\n",
    "      temp_model = self._add_conv_transpose_layer(layer,temp_model)\n",
    "\n",
    "    return temp_model\n",
    "\n",
    "  def _add_conv_transpose_layer(self,layer,temp_model):\n",
    "    \"\"\"Add a single transpose conv layer to the model \"\"\"\n",
    "\n",
    "    layer_num = self._num_conv_layers - layer\n",
    "\n",
    "    conv_transpose_layer = Conv2DTranspose(\n",
    "        filters = self.conv_filters[layer],\n",
    "        kernel_size=self.conv_kernels[layer],\n",
    "        strides=self.conv_strides[layer],\n",
    "        padding=\"same\",\n",
    "        name=\"decoder_conv_transpose_layer_\"+str(layer)\n",
    "        )\n",
    "    \n",
    "    temp_model = conv_transpose_layer(temp_model)\n",
    "    temp_model = ReLU(name =\"decoder_ReLU_transpose_layer_\"+str(layer))(temp_model)\n",
    "    temp_model = BatchNormalization(name = f\"decoder_BN_transpose_layer_\"+str(layer))(temp_model)\n",
    "\n",
    "    return temp_model\n",
    "\n",
    "  def _add_decoder_output(self, temp_model):\n",
    "\n",
    "     conv_transpose_layer = Conv2DTranspose(\n",
    "            filters=1,\n",
    "            kernel_size=self.conv_kernels[0],\n",
    "            strides=self.conv_strides[0],\n",
    "            padding=\"same\",\n",
    "            name=\"decoder_conv_transpose_layer_\"+str(self._num_conv_layers)\n",
    "        )\n",
    "     temp_model = conv_transpose_layer(temp_model)\n",
    "     output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(temp_model)\n",
    "    \n",
    "     return output_layer\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   AUTOENCODER\n",
    "  #----------------------------------------------\n",
    "  def _build_autoencoder(self):\n",
    "    model_input = self._model_input\n",
    "    model_output = self.Decoder(self.Encoder(model_input))\n",
    "    self.Model = Model(model_input, model_output, name=\"Variational_AutoEncoder(VAE)\")\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #            COMPILATION AND TRAINING\n",
    "  #----------------------------------------------\n",
    "  def compile(self, loss_function = \"MSE\",learning_rate=0.0001):\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.Model.compile(optimizer=optimizer,\n",
    "                              loss=self._calculate_combined_loss,\n",
    "                              metrics=[self._calculate_reconstruction_loss, self._calculate_kl_loss] #Metrics are evaluated by the model during training and testing.\n",
    "                              )\n",
    "  \n",
    "  def _calculate_combined_loss(self, y_target, y_predicted):\n",
    "    \"\"\"\n",
    "    the reconstruction loss weight may be optimized during the training process or using some hyperparameter optimization method\n",
    "    \"\"\"\n",
    "    reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
    "    kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
    "    combined_loss = self.reconstruction_loss_weight * reconstruction_loss + kl_loss\n",
    "    return combined_loss\n",
    "\n",
    "  def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
    "    error = y_target - y_predicted\n",
    "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
    "    return reconstruction_loss\n",
    "\n",
    "  def _calculate_kl_loss(self, y_target, y_predicted):\n",
    "    \"\"\"\n",
    "    The main objetive of KL loss is to take the resulting multivariational distribution the closest possible to\n",
    "    a standard normal distribution. It also improves the symmetry around the origin, which helps to avoid gaps in the\n",
    "    latent space.\n",
    "    \"\"\"\n",
    "    kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) - K.exp(self.log_variance), axis=1)\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "  def train(self, x_train, batch_size, num_epochs,porcentaje_validacion):\n",
    "        History= self.Model.fit(x_train,\n",
    "                       x_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=num_epochs,\n",
    "                       validation_split=porcentaje_validacion,\n",
    "                       shuffle=True)\n",
    "        return History\n",
    "  #----------------------------------------------\n",
    "  #                   STRUCTURE SUMMARY\n",
    "  #----------------------------------------------\n",
    "\n",
    "  def summary(self):\n",
    "      self.Encoder.summary()\n",
    "      self.Decoder.summary()\n",
    "      self.Model.summary()\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   SAVE THE MODEL\n",
    "  #----------------------------------------------\n",
    "  def save(self, save_folder=\".\"):\n",
    "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
    "        self._save_parameters(save_folder)\n",
    "        self._save_weights(save_folder)\n",
    "\n",
    "  def _create_folder_if_it_doesnt_exist(self, folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "  def _save_parameters(self, save_folder):\n",
    "      parameters = [\n",
    "          self.input_shape,\n",
    "          self.conv_filters,\n",
    "          self.conv_kernels,\n",
    "          self.conv_strides,\n",
    "          self.latent_space_dim\n",
    "      ]\n",
    "      save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
    "      with open(save_path, \"wb\") as f:\n",
    "          pickle.dump(parameters, f)\n",
    "\n",
    "  def _save_weights(self, save_folder):\n",
    "      save_path = os.path.join(save_folder, \"weights.h5py\")\n",
    "      self.Model.save_weights(save_path)\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   LOAD THE MODEL\n",
    "  #----------------------------------------------\n",
    "  def load_weights(self, weights_path):\n",
    "    self.Model.load_weights(weights_path)\n",
    "\n",
    "  @classmethod\n",
    "  def load(cls, save_folder=\".\"):\n",
    "    parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
    "    with open(parameters_path, \"rb\") as f:\n",
    "        parameters = pickle.load(f)\n",
    "    autoencoder = VAE(*parameters)\n",
    "    weights_path = os.path.join(save_folder, \"weights.h5py\")\n",
    "    autoencoder.load_weights(weights_path)\n",
    "    return autoencoder\n",
    "\n",
    "  #----------------------------------------------\n",
    "  #                   PREDICTION\n",
    "  #----------------------------------------------\n",
    "  def reconstruct(self, images):\n",
    "    latent_representations = self.Encoder.predict(images)\n",
    "    reconstructed_images = self.Decoder.predict(latent_representations)\n",
    "    return reconstructed_images, latent_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_VAE = VAE(\n",
    "            input_shape=(input_shape[0],input_shape[1], 1), \n",
    "            conv_filters=(32, 64, 64, 64),  \n",
    "            conv_kernels=(3, 3, 3, 3),   \n",
    "            conv_strides=(1, 2, 2, 1),  \n",
    "            latent_space_dim = 3\n",
    "            )\n",
    "autoencoder_VAE.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_VAE = VAE.load(\".../VAE3(48)\")  #load model WINDOWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 70\n",
    "EPOCHS = 60\n",
    "percent_validacion=0.2 \n",
    "\n",
    "# Compiling\n",
    "autoencoder_VAE.compile(LEARNING_RATE)\n",
    "\n",
    "# Training\n",
    "History=autoencoder_VAE.train(x_train,BATCH_SIZE,EPOCHS,percent_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train=History.history[\"_calculate_reconstruction_loss\"]\n",
    "error_train_save=np.save('.../train_VAE3_transfers(60).npy',error_train)\n",
    "\n",
    "error_validation=History.history[\"val__calculate_reconstruction_loss\"]\n",
    "error_validation_save=np.save('.../val_VAE3_transfers(60).npy',error_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_VAE.save(\".../VAE3_jtransfer60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_VAE = AutoEncoder.load(\".../VAE3(48)\")  \n",
    "autoencoder_VAE.summary()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(\"TRAIN DATA\")\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epochs')\n",
    "plt.plot(History.history[\"_calculate_reconstruction_loss\"], label=\"Training reconstruction_Loss\")\n",
    "plt.plot(History.history[\"val__calculate_reconstruction_loss\"], label=\"Validation reconstructio Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###---------------------------------------------------------------------------------------\n",
    "#                              TEST IMAGE PROCESSING\n",
    "###---------------------------------------------------------------------------------------\n",
    "N=10                                        \n",
    "original_test_image=x_test[N,:]                   \n",
    "original_image= np.float32(original_test_image)  \n",
    "\n",
    "###---------------------------------------------------------------------------------------\n",
    "#                              PERFORMANCE MEASUREMENT\n",
    "###---------------------------------------------------------------------------------------\n",
    "\n",
    "RESULT =autoencoder_VAE.reconstruct(x_test)  \n",
    "aux=RESULT[0]   \n",
    "output_image=aux[N,:] \n",
    "\n",
    "# # MSE\n",
    "\n",
    "MSE=skimage.measure.compare_mse(original_image,output_image)\n",
    "print('The MSE is:',MSE)\n",
    "\n",
    "# # # SSIM\n",
    "SSIM=skimage.measure.compare_ssim(original_image,output_image,multichannel=True)\n",
    "print('The SSIM figure is:',SSIM)\n",
    "\n",
    "###---------------------------------------------------------------------------------------\n",
    "#                             SPECTROGRAM PLOTTING\n",
    "###---------------------------------------------------------------------------------------\n",
    "\n",
    "time_axis   =   np.load('.../frecuencia.npy') \n",
    "freq_axis   =   np.load('.../tiempo.npy')\n",
    "\n",
    "#El tamaÃ±o de estos ejes se adecuada a la longitud de las imagenes.\n",
    "time_axis   =   time_axis[0:128]\n",
    "freq_axis   =   freq_axis[0:196]\n",
    "\n",
    "original_image  =   original_image[:,:,0]     # (128,196)\n",
    "reconstruction  =   output_image[:,:,0]   # (128,196)\n",
    "\n",
    "# Creando figura \n",
    "plt.figure(figsize=(12,4),dpi=150)\n",
    "plt.subplot(1,2,1)\n",
    "plt.text(0.8,26000,\"Spectrograms\", fontsize=12)\n",
    "plt.text(0.4,-4000,\"(MSE=\"+str(MSE)+\"); SSIM=\"+str(SSIM)+\"); N=\"+str(len(x_train[:,1,1]))+\")\",fontsize=10)\n",
    "plt.title(\"Original TEST image\")\n",
    "Pxx_original=plt.pcolormesh(freq_axis, time_axis, original_image, cmap='Spectral')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Reconstruction\")\n",
    "Pxx_reconstruida=plt.pcolormesh(freq_axis, time_axis, reconstruction, cmap='Spectral')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "generation_code=RESULT[1]\n",
    "print(np.shape(generation_code))\n",
    "M=random.randint(0, len(generation_code[:,0]))\n",
    "print(M)\n",
    "generation_code=generation_code[M,:]\n",
    "print(generation_code)\n",
    "print(np.shape(generation_code),type(generation_code))\n",
    "\n",
    "generation_code_tensor=tf.expand_dims(generation_code, axis=0) #Esta funciÃ³n me devuelve un tensor, asique se debe pasar a array.\n",
    "generation_code_tensor_final= tf.compat.v1.Session().run(generation_code_tensor)\n",
    "\n",
    "Imagen_final = autoencoder_VAE.Decoder.predict(generation_code_tensor_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
